{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all imports for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from datetime import datetime as dt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preping data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the root path of the data? - default is flower_data -> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using root path: \tflower_data\n",
      "    Using train path: \tflower_data/train\n",
      "    Using valid path: \tflower_data/valid\n",
      "    Using test path: \tflower_data/test\n"
     ]
    }
   ],
   "source": [
    "data_dir = input('What is the root path of the data? - default is flower_data ->')\n",
    "if data_dir == '' or data_dir.lower() == 'default': data_dir = 'flower_data'\n",
    "train_dir = data_dir +'/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'\n",
    "print(\n",
    "'    Using root path: \\t{}\\n\\\n",
    "    Using train path: \\t{}\\n\\\n",
    "    Using valid path: \\t{}\\n\\\n",
    "    Using test path: \\t{}'.format(data_dir, train_dir, valid_dir, test_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Your test images have been transformed.\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "means = [0.485, 0.456, 0.406]\n",
    "stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "# TODO: Define your transforms for the training, validation, and testing sets\n",
    "train_data_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=means, std=stds)])\n",
    "\n",
    "test_data_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=means, std=stds)])\n",
    "\n",
    "valid_data_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=means, std=stds)])\n",
    "\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "train_image_datasets = datasets.ImageFolder(train_dir, transform=train_data_transforms)\n",
    "test_image_datasets = datasets.ImageFolder(train_dir, transform=test_data_transforms)\n",
    "valid_image_datasets = datasets.ImageFolder(test_dir, transform=valid_data_transforms)\n",
    "class_to_tensoridx_dict = train_image_datasets.class_to_idx\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "train_dataloaders = torch.utils.data.DataLoader(train_image_datasets, batch_size=40, shuffle=True)\n",
    "test_dataloaders = torch.utils.data.DataLoader(test_image_datasets, batch_size=40, shuffle=True)\n",
    "valid_dataloaders = torch.utils.data.DataLoader(valid_image_datasets, batch_size=16, shuffle=True)\n",
    "                                           \n",
    "dataloaders = {'train': train_dataloaders, 'test': test_dataloaders, 'valid': valid_dataloaders}\n",
    "                                           \n",
    "print('Great! Your test images have been transformed.')\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In what device do you want to run this model, cuda or cpu?  cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks! You selected to run the model using the cuda.\n"
     ]
    }
   ],
   "source": [
    "def device_validation():\n",
    "    device = input('In what device do you want to run this model, cuda or cpu? ->')\n",
    "    if device.lower() in ['cpu', 'cuda']:\n",
    "        print('Thanks! You selected to run the model using the {}.'.format(device))\n",
    "        return device\n",
    "    else:\n",
    "        print('Warning! Wrong input. Choose cuda or cpu.')\n",
    "        return device_validation()\n",
    "device = device_validation()\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many epoch do you want to run? -enter int value  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks! You have selected to run 1 epochs.\n"
     ]
    }
   ],
   "source": [
    "def epoch_validation():\n",
    "    try:\n",
    "        epochs = int(input('How many epoch do you want to run? \\\n",
    "        \\nenter int value ->'))\n",
    "        print('Thanks! You have selected to run {} epochs.'.format(epochs))\n",
    "        return epochs\n",
    "    except:\n",
    "        print('Warning! Enter only integer values')   \n",
    "        return epoch_validation()\n",
    "    \n",
    "epochs = epoch_validation()\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many steps before printing the epoch and loss? - enter int value  40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks! You have selected to print the epochs and loss on every 40 steps.\n"
     ]
    }
   ],
   "source": [
    "def print_sequence_validation():\n",
    "    try:\n",
    "        print_sequence = int(input('How many steps before printing the epoch and loss? \\\n",
    "        \\nenter int value ->'))\n",
    "        print('Thanks! You have selected to print the epochs and loss on every {} steps.'.format(print_sequence))\n",
    "        return print_sequence\n",
    "    except:\n",
    "        print('Warning! Enter only integer values')\n",
    "        return print_sequence_validation()\n",
    "    \n",
    "print_sequence = print_sequence_validation()  \n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the learning rate desired? - enter float value  .001\n"
     ]
    }
   ],
   "source": [
    "def learning_rate_validation():\n",
    "    try:\n",
    "        learning_rate = float(input('What is the learning rate desired? \\\n",
    "        \\nenter float value ->'))\n",
    "        print('Thanks! You have selected use a learning rate of {:f} steps.'.format(learning_rate))\n",
    "        return learning_rate\n",
    "    except:\n",
    "        print('Warning! Enter only float numbers')\n",
    "        return learning_rate_validation()\n",
    "learning_rate = learning_rate_validation()\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! We have two great models to run: 1 for densenet121 or 2 for vgg19_bn \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Which model will you choose, 1 or 2? 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! You have selected the densenet121 architecture.\n",
      "The model input layer is 1024\n"
     ]
    }
   ],
   "source": [
    "def choose_model():\n",
    "    print('Great! We have two great models to run: 1 for densenet121 or 2 for vgg19_bn')\n",
    "    model_selected = str(input('Which model will you choose, 1 or 2? ->'))\n",
    "    if model_selected in ['1', 'densenet121']: \n",
    "        model = models.densenet121(pretrained=True)\n",
    "        input_features = model.classifier.in_features        \n",
    "        print('Great! You have selected the densenet121 architecture.')\n",
    "        print('The model input layer is', input_features)\n",
    "        return model, input_features\n",
    "    elif model_selected in ['2', 'vgg19_bn']: \n",
    "        model = models.vgg19_bn(pretrained=True)\n",
    "        input_features = model.classifier[0].in_features\n",
    "        print('Great! You have selected the vgg19_bn architecture.')\n",
    "        print('The model input layer is', input_features)\n",
    "        return model, input_features\n",
    "    else:\n",
    "        print('Wanring! Please select a valid architecture: 1 for densenet121 or 2 for vgg19_bn.')\n",
    "        return choose_model()\n",
    "\n",
    "model, input_features = choose_model()   \n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer_generator(hidden_layer_qty, model_input_features, hidden_layer_inputs, output_dim):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_layer_qty: the number of hidden layers -required\n",
    "    model_input_features: the model imput features comming from the selected model -required\n",
    "    hidden_layer_inputs: the hidden layer imputs in a list [input_int, input_int2] -required\n",
    "    output_dim: the dimensions of the LogSoftmax output -default 1\n",
    "    \n",
    "    returns the model classifier\n",
    "        assign it by using model.classifier = hidden_layer_generator(parameters_here)\n",
    "    \"\"\"\n",
    "    if hidden_layer_qty == len(hidden_layer_inputs):\n",
    "        \n",
    "        # initiating layer list\n",
    "        hidden_layers = [('fc1', nn.Linear(input_features , hidden_layer_inputs[0])), \n",
    "                         ('relu', nn.ReLU()),\n",
    "                         ('dropout', nn.Dropout(p=0.5))]\n",
    "        \n",
    "        # generating hidden layers and output  \n",
    "        for number in range(hidden_layer_qty):\n",
    "            fc = number + 2\n",
    "            layer_name = ('fc%s' % fc)\n",
    "            try:\n",
    "                layer = nn.Linear(hidden_layer_inputs[number], hidden_layer_inputs[number+1])\n",
    "            except:\n",
    "                layer = nn.Linear(hidden_layer_inputs[number], 121)\n",
    "            hidden_layers.append((layer_name, layer))\n",
    "            if number < range(hidden_layer_qty)[-1]:\n",
    "                hidden_layers.append(('relu', nn.ReLU()))\n",
    "                hidden_layers.append(('dropout', nn.Dropout(p=0.5)))\n",
    "            else:\n",
    "                hidden_layers.append(('output', nn.LogSoftmax(dim=output_dim)))\n",
    "\n",
    "    else:\n",
    "        print('The lenght of the list of hidden layers does not equal the quantity of layers.')\n",
    "        \n",
    "    return hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many layers in the model? enter int number  2\n",
      "Enter a list of hidden inputs separated by 1 space E.g. 500 300 120 500  300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Unexpedted input character, please enter integers\n",
      "Let's try that again: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many layers in the model? enter int number  2\n",
      "Enter a list of hidden inputs separated by 1 space E.g. 500 300 120 500, 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Unexpedted input character, please enter integers\n",
      "Let's try that again: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many layers in the model? enter int number  2\n",
      "Enter a list of hidden inputs separated by 1 space E.g. 500 300 120 500 300\n",
      "How many dimensions in the LogSoftmax output? enter int number  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Your classifier is ready:\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=1024, out_features=500, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc2): Linear(in_features=500, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=121, bias=True)\n",
      "  (output): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def layer_validation():\n",
    "    try:\n",
    "        hidden_layer_qty = int(input('How many layers in the model? \\\n",
    "        \\nenter int number ->'))\n",
    "        hidden_layer_inputs = input('Enter a list of hidden inputs separated by 1 space \\\n",
    "        \\n(E.g. 500 300 120) ->').split(' ')\n",
    "        hidden_layer_inputs = [int(num) for num in hidden_layer_inputs]\n",
    "        \n",
    "        # quantity of layers vs hidden layer list validation\n",
    "        if hidden_layer_qty != len(hidden_layer_inputs):            \n",
    "            print('Warning! The lenght of the list of hidden layers does not equal the quantity of layers.')\n",
    "            print(\"Let's try that again: \\n\")\n",
    "            return layer_validation()\n",
    "        \n",
    "        ouput_dim = int(input('How many dimensions in the LogSoftmax output? \\\n",
    "        \\nenter int number ->'))\n",
    "        return hidden_layer_qty, hidden_layer_inputs, ouput_dim \n",
    "    except:\n",
    "        print('Warning! Unexpedted input character, please enter integers')\n",
    "        print(\"Let's try that again: \\n\")\n",
    "        return layer_validation()\n",
    "\n",
    "hidden_layer_qty, hidden_layer_inputs, ouput_dim = layer_validation()\n",
    "\n",
    "hidden_layers = hidden_layer_generator(hidden_layer_qty, \n",
    "                                                input_features, \n",
    "                                                hidden_layer_inputs, \n",
    "                                                ouput_dim)\n",
    "\n",
    "ordered_dict = OrderedDict(hidden_layers)\n",
    "\n",
    "classifier = nn.Sequential(ordered_dict)\n",
    "\n",
    "print('-')\n",
    "print('\\nGreat! Your classifier is ready:')\n",
    "print(classifier)\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TickTock:\n",
    "    \"\"\"\n",
    "    Automatic timer. Assign it to a variable and call the stop_clock method.\n",
    "    Returns a string format '0:00:00.000000'\n",
    "    \"\"\"\n",
    "    def __init__(self):        \n",
    "        self.start_time = dt.now()\n",
    "    def stop_clock(self):\n",
    "        td = dt.now() - self.start_time\n",
    "        return ':'.join(str(td).split(':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Model and Saving Neural Network Sequential Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must change ordered_dict to actual classifier\n",
    "model_settings = {'model': model, 'sequential_arg': ordered_dict}\n",
    "with open('model_settings.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_settings, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is now training. :)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-081397ab9a7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The model is now training. :)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Freeze parameters so we don't backprop through them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print('The model is now training. :)')\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier = classifier\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "timer = TickTock()\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for steps, (images, labels) in enumerate(dataloaders['train']):\n",
    "\n",
    "        # move images and labels to device selected\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                test_loss, accuracy = validation(model, dataloaders['test'], criterion)\n",
    "            \n",
    "            \n",
    "            print(\"Epoch: {}/{} -|- \".format(e+1, epochs),\n",
    "                  \"Train Loss: {:.4f} -|- \".format(running_loss/print_every),\n",
    "                  \"Test Loss: {:.3f} -|- \".format(test_loss/len(dataloaders['test'])),\n",
    "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(dataloaders['test'])))\n",
    "            running_loss = 0   \n",
    "\n",
    "time_delta = timer.stop_clock()\n",
    "print('Training time:', time_delta)\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Mappings and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have saved a pickle file of classes to tensor index mappings and a checkpoint of the model.\n"
     ]
    }
   ],
   "source": [
    "with open('class_to_tensoridx_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(class_to_tensoridx_dict, handle)\n",
    "\n",
    "torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "print('We have saved a pickle file of classes to tensor index mappings and a checkpoint of the model.')\n",
    "print('We have dump the model_settings pickle file to use in the prediciton.')\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Accuracy of Model with Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now checking the accuracy of the model. :)\n",
      "Training is done.\n",
      "Here is the accuracy of the network on test images: 64 %\n"
     ]
    }
   ],
   "source": [
    "print('We are now checking the accuracy of the model. :)')\n",
    "correct = 0\n",
    "total = 0\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    for (images, labels) in test_dataloaders:\n",
    "        # move images and labels to device selected\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print('Training is done.')\n",
    "print('Here is the accuracy of the network on test images: %d %%' %\n",
    "     (100 * correct / total)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
